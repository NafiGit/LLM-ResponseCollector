# Response collector - LLM Software Package Hallucination Research
- The script provided is a response collector for conducting software package hallucination research using the LLM (Large Language Model).
- It automates the process of generating responses from LLMs with different parameter combinations for provided prompts.
- The generated responses are collected and stored for further analysis and evaluation in the context of hallucination research.

## Models
- Gemma
    - 7B
    - 2b
- To Add
    - Mistral 
    - Llama

## Docs
- `main.py`
    - Checks for the presence of the langchain_community package and installs it if not found.
    - Defines functions load_json() and save_json() for safe loading and saving of JSON data respectively.
    - Defines file paths for input (prompts and models) and output (responses).
    - Loads prompts and model configurations from JSON files specified by the file paths.
    - Initializes an empty list response_data to store response details.
    - Iterates over each prompt loaded.
    - For each prompt, iterates over each model and its parameters loaded.
    - Within nested loops, iterates over each combination of parameters for the current model.
    - Initializes an Ollama model with the current parameter combination.
    - Invokes the model to generate a response for the current prompt.
    - Catches any exceptions that might occur during model invocation, prints an error message, and continues to the next iteration.
    - Collects necessary details such as prompt text, model name, parameters, and response into a dictionary.
    - Appends the response details dictionary to the response_data list.
    - Prints the response details and a message indicating that the response is saved to the JSON file (response.json) after each iteration.
    - Saves the accumulated response data to the response.json file progressively.
    - Prints a message indicating that all responses are saved to response.json after processing all prompts.

- `models.json`
    - Top-p (top_p):
        - Values: Eg: [0.2, 0.5, 0.9]
        - Represents the probability threshold for nucleus sampling.

    - Top-k (top_k):
        - Values: Eg: [0.2, 0.5, 0.9]
        - Represents the size of the top-k candidates to consider.

    - Temperature (temp):
        - Values: Eg: [0.2, 0.5, 0.9]
        - Controls the randomness of the output generated by the model.

